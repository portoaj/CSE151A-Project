# CSE151A-Project

## Data Exploration and Preprocessing
Here's a link to my noteboook for data processing [Notebook here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb)
### Labeled Data
My labeled data was created by myself by scraping images of keyboards from popular mechanical keyboard vendors like KBDFans, and manually drawing bounding boxes over each keycap in the image. Here's an example: ![Annotation Image](https://github.com/portoaj/CSE151A-Project/raw/main/Examples/dataannotationexample.png)

Each of those neon yellow boxes around the keycaps were those bounding boxes that I manually drew. I draw bounding boxes for around a hundred keycaps each for tens of images, which was pretty painstaking but should allow me to train an object detection model capable of finding the keycaps within an image.

Each image was preprocessed to be 640x640, as this is the standard for YOLO models, and has worked well for me in the past. To maintain the aspect ratio the image was stretched and then had white edges added as needed to make each image a 640x640 square. Here's an example: ![Data preprocessing example](https://github.com/portoaj/CSE151A-Project/raw/main/Examples/imageresizing.png)

To augment my data I used a combination of image level and object level data augmentation tools that are built in to roboflow. While I could have done the image level augmentation like changing the saturation myself, the object level augmentation would have been very challenging because augmentations like shearing the image would also move where the labels have to be.

After looking through the different options, here is what I settled on:

Rotation between -3 and +3 degrees as the images may not always be perfectly oriented upwards. I decided against 90 degree rotations as I think it's reasonable to expect my input images to be facing up.

Shear +- 5 degrees horizontally and vertically. If the image isn't aligned perfectly or there is some distortion to the input image, this should help make the image generalize to that.

Saturation +- 10% as some input images may appear more saturated than others depending on the keyboard colors and the camera settings.

Blur of up to a 1px radius as parts of the keyboard could be blurry depending on how the focus on the camera is set.

Further exploration of my labeled data is in the notebook [Here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb)
### Unlabeled Data
I'm not sure if I'm going to be able to use the unlabeled data later in the project as training object detection models from unlabeled data is very challenging, but it will be an interesting exercise to collect it.

To do this I'm going to scrape a popular mechanical keyboard vendor, kbdfans.com. I'm registered as an affiliate with KBDFans as part of my kbdlab.io project and they've given me permission to use their images as part of that program.

One trick I'm going to use to make my web scraping simpler is to use the automatic API generated by they website through shopify. Here's the documentation: https://shopify.dev/docs/api/ajax/reference/product. Interestingly, shopify automatically generates this API for all of their websites, likely to make scraping simpler so that people don't overwhelm their websites with scraping requests.

The code for the scraping is in the notebook [Here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb) 

# Milestone 3
Here's a link to my noteboook milestone 3 [Notebook here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/CSE151AProjectMilestone3.ipynb)

1. In this milestone I show how to preprocess the unlabeled images by downscaling them to 640x640 as well as do some basic image augmentation. I also used image augmentation as well as image normalization later in the milestone on my labeled data before training my model.

2.Done.
3.My training error is slightly lower than my validation error but both are fairly low at around .9 and 1 respectively. This error is a combination of class error (which should be none since I have one class) and the box placement error on the first image of each batch, which can see is quite low.
4.My model seems to fit in the balanced section of the fitting graph. If it was underfitting then you would expect the training error to be quite high but both the training and validation error were quite low. If it was overfitting then you would expect the validation error to rise towards the end of the training but it seems to be slowly decreasing even at the end of the training and is mostly matching the training error although it is slightly larger.
5. The next model that I want to try is the YOLOS model because it is also offered on hugging face and represents a very different architecture from the DETR model that I used for this milestone. DETR is a very large, modern transformer based architecture whereas YOLO models are a more classical approach to Object Detection based mostly on a deep CNN architecture.
The third model I want to try is DETA, which seems to be the generally best performing version of the DETR class of models available on HuggingFace. This also has the advantage of being very similar to my first model so it will be interesting to compare the performance of the two models on my relatively small dataset.
6. Done
7. 
## Conclusion
This model has shown that I'm definitely able to train an Object Detection model on my dataset with some accuracy. However, the results were quite mixed. If you look at the final testing image you can tell that there are some annotations being drawn where there should be none and also that some of the keycaps aren't annotated. However, the majority of the annotations seem accurate.

There's a lot of room for further improvement by trying different data augmentation strategies, different models and different hyperparameters. In particular, I might want to increase the learning rate as the model converged quite slowly.

# Milestone 4
Jupyer notebook link here: [Notebook here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/CSE151AProjectMilestone4.ipynb)

1. Evaluate your data, labels and loss function. Were they sufficient or did you have have to change them.
I kept my data and labels the same with this model. The main difference was that instead of using Facebook's DETR model I used Facebook's Deformable DETR model this time which the authors of the model described as a similar model to DETR but whose attention modules look at less data which supposedly decreased the number of training epochs needed by 10x.
2. Train your second model
Done
3. Evaluate your model compare training vs test error
Oddly enough my testing error was actually lower than my training error. I think this may be due to my testing set being too small as it includes less than 10 images since my overall dataset is fairly small.
The validation error produced by this model was significantly less than that produced by first model and specifically in my evaluation section the validation recall and precision are much higher in this model than my previous one so I was quite happy.
4. Where does your model fit in the fitting graph, how does it compare to your first model?
My model fits in the fitting graph in a similar place to my first model, I think it is actually quite balanced in terms of complexity. The model seemed to have converged since by the end of the training both training and validation loss were mostly flat but I don't think it overfitted since the validation loss was quite low and the validation loss didn't begin increasing towards the end of training.
5. Did you perform hyper parameter tuning? K-fold Cross validation? Feature expansion? What were the results?
I performed some hyper parameter tuning by trying different amounts of training epochs and adjusting the batch size. At first my training epochs were set too small and the model was clearly still converging by the end of the training so I increased the number of epochs the model would train. I also adjusted the batch size down from 8 to 2 because my model was running out of VRAM. Then, I tried a batch size of 4 which left me just enough VRAM but made model training significantly faster and slightly lowered my validation loss.
5. What is the plan for the next model you are thinking of and why?
Originally I'd planned to train a YOLOS model for this milestone but I decided to work on the deformable DETR model since I was having issues with my evaluation code and it was easier to debug with a model more similar to my previous checkpoint. Now that that's working properly, I think I'll probably do the YOLOS model for my next milestone.
6. Update your readme with this info added to the readme with links to the jupyter notebook!
Done
7. Conclusion section: What is the conclusion of your 2nd model? What can be done to possibly improve it? How did it perform to your first and why?
I'm very happy with this model and it performed much much better than my first model. If you look at the annotated images that it generated at the bottom of the notebook, you can tell that there are certain types of images/ keyboards the model performs badly on, but aside from those the model performed extremely well. I think the best way to improve my model from here would definitely be to add more images as my training set is quite small at somewhere in 20-30 images. I think this would remove the cases where my model struggles with "bad images". I could also try more aggressive image augmentation to generate more variations at different scales, colors, etc.