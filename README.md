# CSE151A-Project

## Data Exploration and Preprocessing
Here's a link to my noteboook for data processing [Notebook here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb)
### Labeled Data
My labeled data was created by myself by scraping images of keyboards from popular mechanical keyboard vendors like KBDFans, and manually drawing bounding boxes over each keycap in the image. Here's an example: ![Annotation Image](https://github.com/portoaj/CSE151A-Project/raw/main/Examples/dataannotationexample.png)

Each of those neon yellow boxes around the keycaps were those bounding boxes that I manually drew. I draw bounding boxes for around a hundred keycaps each for tens of images, which was pretty painstaking but should allow me to train an object detection model capable of finding the keycaps within an image.

Each image was preprocessed to be 640x640, as this is the standard for YOLO models, and has worked well for me in the past. To maintain the aspect ratio the image was stretched and then had white edges added as needed to make each image a 640x640 square. Here's an example: ![Data preprocessing example](https://github.com/portoaj/CSE151A-Project/raw/main/Examples/imageresizing.png)

To augment my data I used a combination of image level and object level data augmentation tools that are built in to roboflow. While I could have done the image level augmentation like changing the saturation myself, the object level augmentation would have been very challenging because augmentations like shearing the image would also move where the labels have to be.

After looking through the different options, here is what I settled on:

Rotation between -3 and +3 degrees as the images may not always be perfectly oriented upwards. I decided against 90 degree rotations as I think it's reasonable to expect my input images to be facing up.

Shear +- 5 degrees horizontally and vertically. If the image isn't aligned perfectly or there is some distortion to the input image, this should help make the image generalize to that.

Saturation +- 10% as some input images may appear more saturated than others depending on the keyboard colors and the camera settings.

Blur of up to a 1px radius as parts of the keyboard could be blurry depending on how the focus on the camera is set.

Further exploration of my labeled data is in the notebook [Here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb)
### Unlabeled Data
I'm not sure if I'm going to be able to use the unlabeled data later in the project as training object detection models from unlabeled data is very challenging, but it will be an interesting exercise to collect it.

To do this I'm going to scrape a popular mechanical keyboard vendor, kbdfans.com. I'm registered as an affiliate with KBDFans as part of my kbdlab.io project and they've given me permission to use their images as part of that program.

One trick I'm going to use to make my web scraping simpler is to use the automatic API generated by they website through shopify. Here's the documentation: https://shopify.dev/docs/api/ajax/reference/product. Interestingly, shopify automatically generates this API for all of their websites, likely to make scraping simpler so that people don't overwhelm their websites with scraping requests.

The code for the scraping is in the notebook [Here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/DataExploration.ipynb) 

# Milestone 3
Here's a link to my noteboook milestone 3 [Notebook here](https://raw.githubusercontent.com/portoaj/CSE151A-Project/main/CSE151AProjectMilestone3.ipynb)

1. In this milestone I show how to preprocess the unlabeled images by downscaling them to 640x640 as well as do some basic image augmentation. I also used image augmentation as well as image normalization later in the milestone on my labeled data before training my model.

2.Done.
3.My training error is slightly lower than my validation error but both are fairly low at around .7 and .8 respectively. This error is a combination of class error (which should be none since I have one class) and the box placement error on the first image of each batch, which can see is quite low.
4.My model seems to fit in the balanced section of the fitting graph. If it was underfitting then you would expect the training error to be quite high but both the training and validation error were quite low. If it was overfitting then you would expect the validation error to rise towards the end of the training but it seems to be slowly decreasing even at the end of the training and is mostly matching the training error although it is slightly larger.
5. The next model that I want to try is the YOLOS model because it is also offered on hugging face and represents a very different architecture from the DETR model that I used for this milestone. DETR is a very large, modern transformer based architecture whereas YOLO models are a more classical approach to Object Detection based mostly on a deep CNN architecture.
The third model I want to try is DETA, which seems to be the generally best performing version of the DETR class of models available on HuggingFace. This also has the advantage of being very similar to my first model so it will be interesting to compare the performance of the two models on my relatively small dataset.
6. Done
7. 
## Conclusion
This model has shown that I'm definitely able to train an Object Detection model on my dataset with some accuracy. However, the results were quite mixed. If you look at the final testing image you can tell that there are some annotations being drawn where there should be none and also that some of the keycaps aren't annotated. However, the majority of the annotations seem accurate.

There's a lot of room for further improvement by trying different data augmentation strategies, different models and different hyperparameters. In particular, I might want to increase the learning rate as the model converged quite slowly.